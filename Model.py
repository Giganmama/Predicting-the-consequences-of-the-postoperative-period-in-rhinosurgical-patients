# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17avnkIla9FxSBjgi_Oz0JaJ3ZahSR4LG
"""

import pandas as pd
import json

import warnings
warnings.filterwarnings('ignore')

data = pd.read_excel("working.xlsx")
data.head()

pd.set_option('display.max_rows', 100)
pd.set_option('display.max_columns', 100)

# Устанавливаем формат для чисел с плавающей запятой
pd.set_option('display.float_format', lambda x: '%.3f' % x)

# Теперь pandas будет отображать до 100 строк и столбцов, а числа будут отображаться с тремя знаками после запятой

# Устанавливаем первую строку в качестве заголовка
data.columns = data.iloc[0]

# Удаляем первую строку, которая уже стала заголовком
data = data[1:]

# Переводим названия колонок в строковые значения для удобства
data.columns = data.columns.astype(str)

# Optional: reset index if needed
data = data.reset_index(drop=True)

# Указываем столбцы, которые хотим сохранить
columns_to_keep = ["LF/HF", "%VLF", "%LF", "%HF"]

# Создаем новый DataFrame, содержащий только эти столбцы
filtered_data = data[columns_to_keep]

# Показываем результат
filtered_data.head()

lf_hf_data = data.filter(regex="LF/HF")
vlf_data = data.filter(regex="%VLF")
hf_data = data.filter(regex="%HF")
lf_data = data.filter(regex="%LF")

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Пример использования уже созданных DataFrame
lf_hf_data1 = lf_hf_data.iloc[:, :-1]
vlf_data1 = vlf_data.iloc[:, :-1]
hf_data1 = hf_data.iloc[:, :-1]
lf_data1 = lf_data.iloc[:, :-1]

# Применяем функцию pd.to_numeric() к каждому элементу Series
lf_data1 = lf_data1.apply(pd.to_numeric, errors='coerce')
lf_hf_data1 = lf_hf_data1.apply(pd.to_numeric, errors='coerce')
vlf_data1 = vlf_data1.apply(pd.to_numeric, errors='coerce')
hf_data1 = hf_data1.apply(pd.to_numeric, errors='coerce')

# Удаляем строки с NaN, если они появились после преобразования
lf_data1 = lf_data1.dropna()
lf_hf_data1 = lf_hf_data1.dropna()
vlf_data1 = vlf_data1.dropna()
hf_data1 = hf_data1.dropna()

# Удаляем знак процента из имен столбцов
lf_data1.columns = [col.replace('%', '') for col in lf_data1.columns]
lf_hf_data1.columns = [col.replace('%', '') for col in lf_hf_data1.columns]
vlf_data1.columns = [col.replace('%', '') for col in vlf_data1.columns]
hf_data1.columns = [col.replace('%', '') for col in hf_data1.columns]

# Разделяем данные на признаки и целевую переменную
X_lf = lf_data1.iloc[:, :-1]
y_lf = lf_data1.iloc[:, -1]
X_lf_hf = lf_hf_data1.iloc[:, :-1]
y_lf_hf = lf_hf_data1.iloc[:, -1]
X_vlf = vlf_data1.iloc[:, :-1]
y_vlf = vlf_data1.iloc[:, -1]
X_hf = hf_data1.iloc[:, :-1]
y_hf = hf_data1.iloc[:, -1]

# Разделяем данные на обучающую и тестовую выборки
X_train_lf, X_test_lf, y_train_lf, y_test_lf = train_test_split(X_lf, y_lf, test_size=0.2, random_state=42)
X_train_lf_hf, X_test_lf_hf, y_train_lf_hf, y_test_lf_hf = train_test_split(X_lf_hf, y_lf_hf, test_size=0.2, random_state=42)
X_train_vlf, X_test_vlf, y_train_vlf, y_test_vlf = train_test_split(X_vlf, y_vlf, test_size=0.2, random_state=42)
X_train_hf, X_test_hf, y_train_hf, y_test_hf = train_test_split(X_hf, y_hf, test_size=0.2, random_state=42)

# Инициализируем и обучаем модели XGBoost
xg_reg_lf_data1 = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,
                                   max_depth=5, alpha=10, n_estimators=10)
xg_reg_lf_data1.fit(X_train_lf, y_train_lf)

xg_reg_lf_hf_data1 = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,
                                      max_depth=5, alpha=10, n_estimators=10)
xg_reg_lf_hf_data1.fit(X_train_lf_hf, y_train_lf_hf)

xg_reg_vlf_data1 = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,
                                    max_depth=5, alpha=10, n_estimators=10)
xg_reg_vlf_data1.fit(X_train_vlf, y_train_vlf)

xg_reg_hf_data1 = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,
                                   max_depth=5, alpha=10, n_estimators=10)
xg_reg_hf_data1.fit(X_train_hf, y_train_hf)



# Пример использования функции
models = {
    "LF/HF": xg_reg_lf_hf_data1,
    "VLF": xg_reg_vlf_data1,
    "LF": xg_reg_lf_data1,
    "HF": xg_reg_hf_data1
}


def predict_new_data(new_data):
    # Преобразование всех столбцов в числовой формат
    new_data = new_data.apply(pd.to_numeric, errors='coerce')

    # Выводим названия столбцов
    print("Названия столбцов в данных:", new_data.columns)

    # Проверка наличия необходимых столбцов
    required_columns = ["LF/HF", "VLF", "LF", "HF"]
    missing_columns = [col for col in required_columns if col not in new_data.columns]
    if missing_columns:
        print(f"Отсутствующие столбцы: {missing_columns}")
        return None

    # Выполнение предсказаний для каждого столбца
    predictions = {}
    for col in required_columns:
        if col in new_data.columns and col in models:
            data_to_predict = new_data[[col]]
            predictions[col] = models[col].predict(data_to_predict)
        else:
            predictions[col] = f"Column '{col}' not found in the data or model not provided"

    return predictions


def answer(file_path):
    # Чтение данных из файла
    with open(file_path, 'r') as file:
        data = json.load(file)["data"]

    # Извлечение нужных значений и преобразование в DataFrame
    relevant_data = {
        "LF/HF": data["LF/HF"],
        "VLF": data["VLF"],
        "LF": data["LF"],
        "HF": data["HF"]
    }

    new_data = pd.DataFrame([relevant_data])

    # Вызов функции предсказания
    new_predictions = predict_new_data(new_data)

    if new_predictions is None:
        return None

    # Формирование списка li
    li = [[pred, round(float(val[0]), 1)] for pred, val in new_predictions.items()]
    print(li)
    return li


result = answer('data.json')
print(result)

